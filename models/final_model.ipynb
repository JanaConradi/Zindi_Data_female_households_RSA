{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of % female-headed households in RSA using Gradient Boosting algorithms\n",
    "\n",
    "This notebook contains:\n",
    "1. Testing different boosting algorithms\n",
    "2. Optimization of the best algorithm with GridSearch => XGBoost\n",
    "3. Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Testing different boosting algorithms\n",
    "\n",
    "To obtain the best prediction model, different gradient boosting models are tested. For this task, we chose AdaBoost, GBDT and XGBoost. The selected features derive from the feature importance in a random forest model ('psa_00', 'car_01', 'pw_00', 'lln_01', 'pg_00')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import  GradientBoostingRegressor\n",
    "from xgboost import XGBRFRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "#df = pd.read_csv(\"/Users/janaconradi/neuefische/Zindi_Data_female_households_RSA/data/Train.csv\")\n",
    "df = pd.read_csv(\"../data/Train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X (with selected values) and y values\n",
    "X = df[['psa_00', 'car_01', 'pw_00', 'lln_01', 'pg_00']]\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE AdaBoost: 4.31\n",
      "R2 AdaBoost: 0.81\n",
      "Improvement to baseline model: 30.85%\n",
      "RMSE GradientBoostingRegressor: 3.9\n",
      "R2 GradientBoostingRegressor: 0.85\n",
      "Improvement to baseline model: 37.31%\n",
      "RMSE XGBRFRegressor: 3.86\n",
      "R2 XGBRFRegressor: 0.85\n",
      "Improvement to baseline model: 37.93%\n"
     ]
    }
   ],
   "source": [
    "# train Boosting regressor\n",
    "ada = AdaBoostRegressor(random_state=RSEED, n_estimators=100)\n",
    "gbdt = GradientBoostingRegressor(random_state=RSEED)\n",
    "xgb = XGBRFRegressor(random_state=RSEED)\n",
    "\n",
    "regressors = {\"AdaBoost\": ada, \"GradientBoostingRegressor\": gbdt, \"XGBRFRegressor\": xgb}\n",
    "\n",
    "for model, regressor in regressors.items():\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    print(f\"RMSE {model}: {round(mean_squared_error(y_test, y_pred, squared=False),2)}\")\n",
    "    print(f\"R2 {model}: {round(r2_score(y_test, y_pred),2)}\")\n",
    "    improvement = -100 * (round(mean_squared_error(y_test, y_pred, squared=False),3)-6.227)/6.227\n",
    "    print('Improvement to baseline model: {:0.2f}%'.format(improvement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "XGBoost archieves the lowest RMSE value (3.88) and the highest R2 value (0.85) and will therefore be chosen as a suitable predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimizing the XGBoost model\n",
    "\n",
    "As XGBoost archieved the best results, the model will be further tweaked and evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch and Crossvalidation for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameter grid (as dictionary)\n",
    "param_grid = {'gamma': [0, 1, 10, 100],\n",
    " 'max_depth': [4, 5, 6, 8],\n",
    " 'min_child_weight': [1, 2]\n",
    "}\n",
    "\n",
    "# Instantiate gridsearch and define the metric to optimize \n",
    "gs = GridSearchCV(XGBRFRegressor(random_state=RSEED), param_grid, scoring='r2',\n",
    "                  cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit gridsearch object to data\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best hyperparameter found by GridSearch\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train XGBoost regressor with best params\n",
    "xgb = XGBRFRegressor(random_state=RSEED, **gs.best_params_)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation metrics\n",
    "print(f\"RMSE: {round(mean_squared_error(y_test, y_pred, squared=False),3)}\")\n",
    "print(f\"R2: {round(r2_score(y_test, y_pred),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The Gridsearch could not further improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Error Analysis\n",
    "\n",
    "Here, we shortly take a look at the distribution of the residual of the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "residuals = y_test - y_pred\n",
    "fig,ax=plt.subplots()\n",
    "sns.scatterplot(y_pred, residuals, ax=ax)\n",
    "ax.set_xlabel('Predicted Values')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_title(\"Residualplot\")\n",
    "ax.axhline()\n",
    "ax.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The residuals of the final model are almost randomly distributed. The model seem to slightly overpredict higher values."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a4186c5bac020f8944d6941ec681f12ae605ca4515fa6285811bd64917683c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
